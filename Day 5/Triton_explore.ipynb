{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c6N2nZbj71V",
        "outputId": "349787ec-959c-471c-ea12-1e9181a6b68c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from triton) (3.17.0)\n",
            "Input x: tensor([0.3990, 0.5167, 0.0249, 0.9401, 0.9459, 0.7967, 0.4150, 0.8203, 0.2290,\n",
            "        0.9096], device='cuda:0')\n",
            "Input y: tensor([0.9722, 0.7910, 0.4690, 0.3300, 0.3345, 0.3783, 0.7640, 0.6405, 0.1103,\n",
            "        0.3594], device='cuda:0')\n",
            "GPU result: tensor([1.3713, 1.3076, 0.4940, 1.2701, 1.2803, 1.1750, 1.1790, 1.4607, 0.3393,\n",
            "        1.2689], device='cuda:0')\n",
            "PyTorch result: tensor([1.3713, 1.3076, 0.4940, 1.2701, 1.2803, 1.1750, 1.1790, 1.4607, 0.3393,\n",
            "        1.2689], device='cuda:0')\n",
            "Test passed!\n"
          ]
        }
      ],
      "source": [
        "# Install Triton (only needed once in Colab)\n",
        "!pip install triton\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch  # For creating and managing tensors (lists of numbers)\n",
        "import triton  # For writing GPU code\n",
        "import triton.language as tl  # For special Triton operations\n",
        "\n",
        "# Set the GPU device (Colab usually has one GPU, so this is \"cuda:0\")\n",
        "DEVICE = torch.device(\"cuda:0\")\n",
        "\n",
        "# Define the kernel (instructions for GPU workers)\n",
        "@triton.jit  # Tells Triton to turn this into GPU code\n",
        "def add_kernel(\n",
        "    x_ptr,  # Pointer to the first list (x) in GPU memory\n",
        "    y_ptr,  # Pointer to the second list (y) in GPU memory\n",
        "    output_ptr,  # Pointer to where we'll store the results\n",
        "    n_elements,  # Total number of numbers in the lists\n",
        "    BLOCK_SIZE: tl.constexpr,  # How many numbers each group of workers handles (set later)\n",
        "):\n",
        "    # Get the ID of this group of workers (called a \"block\")\n",
        "    # Each group gets a unique ID to know which part of the lists to work on\n",
        "    pid = tl.program_id(axis=0)\n",
        "\n",
        "    # Calculate where this group should start working in the lists\n",
        "    # Example: If pid=0 and BLOCK_SIZE=4, start at index 0\n",
        "    #          If pid=1 and BLOCK_SIZE=4, start at index 4\n",
        "    block_start = pid * BLOCK_SIZE\n",
        "\n",
        "    # Create a list of exact positions (indices) this group should work on\n",
        "    # Example: If block_start=4, offsets=[4, 5, 6, 7] (for BLOCK_SIZE=4)\n",
        "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
        "\n",
        "    # Make sure we don't try to work on numbers that don't exist\n",
        "    # Example: If n_elements=5 but offsets goes up to 7, mask=[True, True, True, True, False, ...]\n",
        "    mask = offsets < n_elements\n",
        "\n",
        "    # Load numbers from the first list (x) into fast GPU memory\n",
        "    # Use the mask to avoid loading numbers beyond n_elements\n",
        "    x = tl.load(x_ptr + offsets, mask=mask, other=None)\n",
        "\n",
        "    # Load numbers from the second list (y) into fast GPU memory\n",
        "    # Use the mask to avoid loading numbers beyond n_elements\n",
        "    y = tl.load(y_ptr + offsets, mask=mask, other=None)\n",
        "\n",
        "    # Add the numbers from x and y\n",
        "    # Example: If x[4]=1 and y[4]=6, then output[4]=7\n",
        "    output = x + y\n",
        "\n",
        "    # Save the results back to the output list in GPU memory\n",
        "    # Use the mask to avoid saving beyond n_elements\n",
        "    tl.store(output_ptr + offsets, output, mask=mask)\n",
        "\n",
        "# Define the wrapper (sets up and launches the kernel)\n",
        "def add(x: torch.Tensor, y: torch.Tensor):\n",
        "    # Create an empty list (tensor) to store the results\n",
        "    # It has the same size and type as x\n",
        "    output = torch.empty_like(x)\n",
        "\n",
        "    # Make sure all lists are on the GPU\n",
        "    # If not, show an error with device information\n",
        "    assert x.device == DEVICE and y.device == DEVICE and output.device == DEVICE, \\\n",
        "        f\"Devices don't match: x={x.device}, y={y.device}, output={output.device}, DEVICE={DEVICE}\"\n",
        "\n",
        "    # Get the total number of numbers in the lists\n",
        "    n_elements = output.numel()\n",
        "\n",
        "    # Decide how many groups of workers (blocks) we need\n",
        "    # Example: If n_elements=10 and BLOCK_SIZE=4, we need 3 groups (10/4 = 2.5, rounded up to 3)\n",
        "    grid = lambda meta: (triton.cdiv(n_elements, meta[\"BLOCK_SIZE\"]), )\n",
        "\n",
        "    # Launch the kernel on the GPU\n",
        "    # - Use the grid to decide how many groups to launch\n",
        "    # - Pass the lists (x, y, output), list length (n_elements), and set BLOCK_SIZE=4\n",
        "    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=4)\n",
        "\n",
        "    # Return the result list after the GPU finishes\n",
        "    return output\n",
        "\n",
        "# Test the code with a small example\n",
        "def test_add():\n",
        "    # Set a random seed for reproducibility\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    # Create two small lists (size=10) on the GPU\n",
        "    # Example: x might be [0.5, 0.2, ...], y might be [0.7, 0.1, ...]\n",
        "    size = 10\n",
        "    x = torch.rand(size, device=DEVICE)\n",
        "    y = torch.rand(size, device=DEVICE)\n",
        "\n",
        "    # Run the GPU addition\n",
        "    z_gpu = add(x, y)\n",
        "\n",
        "    # Run PyTorch's addition (the correct answer)\n",
        "    z_cpu = x + y\n",
        "\n",
        "    # Print the inputs and results\n",
        "    print(\"Input x:\", x)\n",
        "    print(\"Input y:\", y)\n",
        "    print(\"GPU result:\", z_gpu)\n",
        "    print(\"PyTorch result:\", z_cpu)\n",
        "\n",
        "    # Check if the GPU result matches PyTorch's result\n",
        "    # Allow small differences due to floating-point math\n",
        "    torch.testing.assert_close(z_gpu, z_cpu, atol=1e-3, rtol=1e-3)\n",
        "    print(\"Test passed!\")\n",
        "\n",
        "# Run the test\n",
        "if __name__ == \"__main__\":\n",
        "    test_add()"
      ]
    }
  ]
}